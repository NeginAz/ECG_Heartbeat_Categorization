





import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split





df_train = pd.read_csv("../Data/mitbih_train.csv", header=None)
df_test = pd.read_csv("../Data/mitbih_test.csv", header=None)





print(df_train.shape)
print(df_test.shape)





print(df_train.head())
print(df_test.head())





print(df_train.iloc[:, -1].value_counts())
print(df_test.iloc[:,-1].value_counts())





df_combined = pd.concat([df_train, df_test])





print(df_combined.iloc[:,-1].value_counts())





overall_stats = df_combined.iloc[:, :-1].describe()

print("Statistical Measures for the Entire Dataset:")
print(overall_stats)

# check for missing values
missing_values = df_combined.isnull().sum()
print("\nMissing values in each colmn:")
print(missing_values[missing_values >0])





import matplotlib.pyplot as plt

label_distribution = df_combined.iloc[:,-1].value_counts()

plt.figure(figsize=(8,6))
plt.bar(label_distribution.index.astype(str), label_distribution.values, color = 'skyblue')
plt.xlabel("Class Label")
plt.ylabel("Number of samples")
plt.title("Distribution of Class Labels")
plt.grid(True)
plt.show()





# Plot random samples from each class
for label in df_combined.iloc[:, -1].unique():
    plt.figure(figsize=(8, 6))
    sample_signals = df_combined[df_combined.iloc[:, -1] == label].iloc[:, :-1].sample(5, random_state=42)
    
    for i, signal in enumerate(sample_signals.values):
        plt.plot(signal, label=f"Sample {i+1}")
    
    plt.xlabel('Time Step (Sample Index)')
    plt.ylabel('ECG Signal Amplitude')
    plt.title(f'ECG Signal Waveforms for Class {label}')
    plt.legend()
    plt.grid(True)
    plt.show()





# Box plots for feature-wise distribution across all samples
# plt.figure(figsize=(15, 8))
# plt.boxplot(df_combined.iloc[:, :-1].values.T, widths=0.6, showfliers=False)
# plt.title('Distribution of ECG Signal Values Across 188 Features')
# plt.xlabel('Feature Index (Time Step)')
# plt.ylabel('ECG Signal Value')
# plt.xticks(rotation=90)
# plt.grid(True)
# plt.show()





from pandas.plotting import autocorrelation_plot

# Autocorrelation for a sample ECG signal from class 0
sample_signal = df_combined[df_combined.iloc[:,-1] == 0] .iloc[0,:-1]

plt.figure(figsize=(8,2))
autocorrelation_plot(sample_signal)
plt.title('Autocorrelation of a Sample ECG Signal (Class 0)')
plt.show()





import numpy as np

# FFT of a sample ECG signal
sample_signal = df_combined.iloc[0, :-1]

# Perform FFT
fft_values = np.fft.fft(sample_signal)
fft_freq = np.fft.fftfreq(len(sample_signal), d=1/125)  # Assuming 125 Hz sampling rate

# Plot the FFT results
plt.figure(figsize=(8, 2))
plt.plot(fft_freq[:len(fft_freq)//2], np.abs(fft_values)[:len(fft_values)//2])
plt.xlabel('Frequency (Hz)')
plt.ylabel('Amplitude')
plt.title('Frequency Domain Analysis of a Sample ECG Signal')
plt.grid(True)
plt.show()





# Add statistical features for each sample
df_features = df_combined.iloc[:, :-1].copy()


df_features['mean'] = df_combined.iloc[:, :-1].mean(axis=1)
df_features['std'] = df_combined.iloc[:, :-1].std(axis=1)
df_features['min'] = df_combined.iloc[:, :-1].min(axis=1)
df_features['max'] = df_combined.iloc[:, :-1].max(axis=1)
df_features['median'] = df_combined.iloc[:, :-1].median(axis=1)
df_features['iqr'] = df_combined.iloc[:, :-1].quantile(0.75, axis=1) - df_combined.iloc[:, :-1].quantile(0.25, axis=1)






from scipy.stats import skew, kurtosis

df_features['zero_crossings'] = df_combined.iloc[:, :-1].apply(lambda x: np.sum(np.diff(np.sign(x)) != 0), axis=1)
df_features['skewness'] = df_combined.iloc[:, :-1].apply(lambda x: skew(x), axis=1)
df_features['kurtosis'] = df_combined.iloc[:, :-1].apply(lambda x: kurtosis(x), axis=1)






# Add frequency domain features
def fft_features(row):
    fft_vals = np.fft.fft(row)
    fft_mag = np.abs(fft_vals)
    return np.mean(fft_mag), np.std(fft_mag), np.max(fft_mag)

df_features[['fft_mean', 'fft_std', 'fft_max']] = df_combined.iloc[:, :-1].apply(lambda row: fft_features(row), axis=1, result_type='expand')



df_features.shape





import os
sys.path.append(os.path.abspath('../src'))

from dimensionality_reduction import DimensionalityReductionVisualization
# Initialize the visualization class with the label column as the last column
# Define a custom component map for each class
n_components_map = {
    3: 2,  # Class 3 with 3 components
    1: 2,  # Class 1 with 2 components
    2: 2,   # Class 2 with 4 components (optional example)
    4: 2,
    0: 2
}
viz = DimensionalityReductionVisualization(label_column=df_combined.columns[-1])

# Plot PCA visualization for all classes individually
viz.plot_all_classes(df_combined, method='pca', n_components_map=n_components_map)

# Plot t-SNE visualization for all classes individually
#viz.plot_all_classes(df_combined, method='tsne', n_components_map=n_components_map)
#viz.plot_tsne(df_combined, target_class=3, n_components=2)






viz.plot_tsne(df_combined, target_class=1, n_components=2)


viz.plot_tsne(df_combined, target_class=2, n_components=2)


viz.plot_tsne(df_combined, target_class=3, n_components=2)





import pandas as pd

def stratified_sample(df: pd.DataFrame, percentage: float, label_col_index: int = -1) -> (pd.DataFrame, pd.DataFrame):
    """
    Perform stratified sampling on a DataFrame without headers, ensuring equal representation of each label.

    Args:
        df (pd.DataFrame): The input DataFrame (no header assumed).
        percentage (float): The percentage of samples to select.
        label_col_index (int): The index of the label column (default is the last column).

    Returns:
        (pd.DataFrame, pd.DataFrame): Sampled and remaining DataFrames.
    """
    # Calculate the total number of samples needed
    total_samples = int(len(df) * (percentage / 100))
    
    # Determine the unique labels and samples per label
    labels = df.iloc[:, label_col_index].unique()  # Specify label column by index
    samples_per_label = total_samples // len(labels)
    
    # Perform stratified sampling
    sampled_dfs = []
    for label in labels:
        label_df = df[df.iloc[:, label_col_index] == label]
        
        # Handle cases where samples_per_label might be greater than available samples
        if len(label_df) < samples_per_label:
            raise ValueError(f"Not enough samples for label {label} to fulfill stratified sampling.")
        
        sampled_label_df = label_df.sample(n=samples_per_label, random_state=42)
        sampled_dfs.append(sampled_label_df)
    
    # Concatenate sampled data and shuffle
    sampled_df = pd.concat(sampled_dfs).sample(frac=1, random_state=42).reset_index(drop=True)
    
    # Get the remaining data by dropping the sampled indices
    remaining_df = df.drop(sampled_df.index).reset_index(drop=True)
    
    return sampled_df, remaining_df



df_test_holdout , df_train = stratified_sample(df_combined, 0.3)

X_train = df_train.iloc[:, :-1]
y_train = df_train.iloc[:, -1]

X_test_holdout = df_test_holdout.iloc[:, :-1]
y_test_holdout = df_test_holdout.iloc[:,-1]


# X_holdout = X_holdout.reset_index(drop=True)
# y_holdout = y_holdout.reset_index(drop=True)


X_test, X_holdout, y_test, y_holdout = train_test_split(X_test_holdout, y_test_holdout , test_size=0.5, random_state=42, stratify=y_test_holdout)



df_combined.head(3)


#%% Split the Dataset:

# from sklearn.model_selection import train_test_split

# # Separate features and labels
# X = df_combined.iloc[:, :-1]
# y = df_combined.iloc[:, -1]

# # Split into training, test, and holdout sets
# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# # Split the remaining data into test and holdout sets (50% each)
# X_test, X_holdout, y_test, y_holdout = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# print(f"Training set size: {X_train.shape[0]}")
# print(f"Test set size: {X_test.shape[0]}")
# print(f"Holdout set size: {X_holdout.shape[0]}")



df_train.shape


print(X_test.shape)
print(X_holdout.shape)

# print(y_test.value_counts())
# print(y_holdout.value_counts())


#%% Split the Dataset:

# from sklearn.model_selection import train_test_split

# # Separate features and labels
# X = df_combined.iloc[:, :-1]
# y = df_combined.iloc[:, -1]

# # Split into training, test, and holdout sets
# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# # Split the remaining data into test and holdout sets (50% each)
# X_test, X_holdout, y_test, y_holdout = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# print(f"Training set size: {X_train.shape[0]}")
# print(f"Test set size: {X_test.shape[0]}")
# print(f"Holdout set size: {X_holdout.shape[0]}")






#%% Repplace 0 (missing values) with Nan

import numpy as np

# Replace zeros with NaN in the training set only
X_train_no_zeros = X_train.replace(0, np.nan)

# Check for NaN values in the training set
missing_values = X_train_no_zeros.isnull().sum().sum()
print(f"Total missing values in the training set (NaN): {missing_values}")





#%% Replace Nan with the row-wise mean

row_means = X_train_no_zeros.mean(axis=1, numeric_only=True)


X_train_imputed = X_train_no_zeros.where(~ X_train_no_zeros.isna(), row_means, axis=0)



# # Check if any NaN values remain
print("Total remaining NaN values in the dataset:")
print(X_train_imputed.isnull().sum().sum())





# Identify the minority classes
class_distribution = y_train.value_counts().sort_values()
minority_classes = class_distribution.index[:3]
print(f"Minority classes: {minority_classes.tolist()}")
print("Class distribution in the training set:")
print(class_distribution)






#%%  
# Define target sample sizes for each minority class
class_sample_map = {
    3.0: 44000,  # Generate 15000 samples for class 3.0
    1.0: 42000,  # Generate 13000 samples for class 1.0
    2.0: 37500   # Generate 7500 samples for class 2.0
}

# Optionally, define custom GMM components for each class
n_components_map = {
    3.0: 2,  # Use 3 components for class 3.0
    1.0: 1,  # Use 5 components for class 1.0
    2.0: 1   # Use 4 components for class 2.0
}








# Import the GMM data augmentation function
import sys
import os

from generate_synthetic_data_gmm import GenerateSyntheticDataGmm

# Generate synthetic data for the specified classes
X_synthetic, y_synthetic = GenerateSyntheticDataGmm(
    pd.concat([X_train_imputed, y_train], axis=1),
    class_sample_map=class_sample_map,
    n_components_map=n_components_map
)
print(f"Generated {X_synthetic.shape[0]} synthetic samples for classes {list(class_sample_map.keys())}")






import pandas as pd

# Concatenate the synthetic samples with the original training data
X_train_augmented = pd.concat([X_train_imputed, X_synthetic], axis=0, ignore_index=True)
y_train_augmented = pd.concat([y_train, y_synthetic], axis=0, ignore_index=True)

print(f"New training set size after GMM augmentation: {X_train_augmented.shape[0]}")
print(f"Number of labels in augmented set: {y_train_augmented.shape[0]}")






#%% Prepare Data
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from cnn_lstm_classifier import CNNLSTMClassifier  # Assuming the model class is in the src directory

# Ensure the data is in numpy array format for Keras
X_data = np.expand_dims(X_train_augmented.values, axis=-1)  # Add channel dimension for CNN
y_data = y_train_augmented.values

print(f"Input shape for model: {X_data.shape}")
print(f"Labels shape: {y_data.shape}")

#%%





import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from tensorflow import keras
from data_augmentation import DataAugmentation  # Assuming the class is in the src folder
from cnn_lstm_classifier import CNNLSTMClassifier  # Assuming the model class is in the src directory

# Initialize Data Augmentation class
augmenter = DataAugmentation(
    shift_max=5, 
    noise_level=0.01, 
    scale_range=(0.9, 1.1)
)

# Define the augmentation factor (e.g., 0.5, 1, 2, etc.)
augmentation_factor = 1  # You can adjust this value as needed

# ðŸ†• Apply data augmentation to the entire dataset before splitting
X_augmented = augmenter.augment_batch(X_data, augmentation_factor=augmentation_factor)

print("Data generated ...")

# Calculate the correct number of labels for the augmented samples
num_original_samples = len(y_data)
num_augmented_samples = X_augmented.shape[0]

# Generate the correct number of labels for the augmented samples
y_augmented = np.repeat(y_data, np.ceil(num_augmented_samples / num_original_samples).astype(int))[:num_augmented_samples]

# Combine original and augmented data
X_combined = np.vstack((X_data, X_augmented))
y_combined = np.concatenate((y_data, y_augmented))

# Shuffle the combined dataset
shuffle_indices = np.random.permutation(X_combined.shape[0])
X_combined = X_combined[shuffle_indices]
y_combined = y_combined[shuffle_indices]

print(f"Combined dataset size: {X_combined.shape[0]}")

# Initialize Stratified K-Fold with 5 splits
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# To store evaluation metrics
accuracy_scores = []
classification_reports = []
confusion_matrices = []

fold_number = 1





for train_index, val_index in kf.split(X_combined, y_combined):
    print(f"\nStarting training for Fold {fold_number}...")
    
    # Split the data into training and validation sets
    X_train_fold, X_val_fold = X_combined[train_index], X_combined[val_index]
    y_train_fold, y_val_fold = y_combined[train_index], y_combined[val_index]
    
    print(f"Training set size: {X_train_fold.shape[0]}")
    print(f"Validation set size: {X_val_fold.shape[0]}")


# Initialize the CNN + LSTM model
    model = CNNLSTMClassifier(
        input_shape=(187, 1),  # The input shape is already correctly set
        num_classes=len(np.unique(y_combined)),
        learning_rate=5e-3,
        batch_size=32,
        epochs=50
    )
    
    # Train the model with the combined data
    model.fit(X_train_fold, y_train_fold, X_val_fold, y_val_fold)
    
    # Evaluate the model using the class's evaluate method
    print(f"\nEvaluating Fold {fold_number}...")
    model.evaluate(X_val_fold, y_val_fold)
    
    # Load the best model to make predictions
    best_model = keras.models.load_model('best_model.h5')
    
    # Predict on the validation set using the best model
    y_val_pred = best_model.predict(X_val_fold)
    y_val_pred_classes = np.argmax(y_val_pred, axis=1)
    
    # Calculate accuracy
    accuracy = accuracy_score(y_val_fold, y_val_pred_classes)
    accuracy_scores.append(accuracy)
    
    print(f"Accuracy for Fold {fold_number}: {accuracy:.4f}")
    
    # Generate classification report and confusion matrix
    classification_reports.append(classification_report(y_val_fold, y_val_pred_classes, digits=4))
    confusion_matrices.append(confusion_matrix(y_val_fold, y_val_pred_classes))
    
    fold_number += 1





model.plot_history()





#%% evlauate the model with the test set

# Ensure the test data has the correct shape
X_test = np.expand_dims(X_test.values, axis=-1)  # Add channel dimension if not already added
print(f"Test set shape: {X_test.shape}")
print(f"Test labels shape: {y_test.shape}")


from tensorflow import keras

# Load the best model weights
best_model = keras.models.load_model('best_model.h5')

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Predict on the test set
y_test_pred = best_model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred, axis=1)

# Generate classification report
print("Classification Report on Test Set:")
print(classification_report(y_test, y_test_pred_classes, digits=4))

# Calculate accuracy
test_accuracy = accuracy_score(y_test, y_test_pred_classes)
print(f"Accuracy on Test Set: {test_accuracy:.4f}")

# Plot confusion matrix
conf_matrix = confusion_matrix(y_test, y_test_pred_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix on Test Set')
plt.show()


import numpy as np
from data_augmentation import DataAugmentation
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow import keras

# Initialize the Data Augmentation class for noise injection
augmenter = DataAugmentation(
    shift_max=0,       # No time shift
    noise_level=0.1,   # Introduce significant noise
    scale_range=(1.0, 1.0), # No scaling
    expected_length=187
)

# ðŸ†• Prepare holdout data with the channel dimension for augmentation
X_holdout_prepared = np.expand_dims(X_holdout.values, axis=-1)  # Shape should be (samples, 187, 1)
y_holdout = y_holdout.values

#%%
print(f"Prepared holdout set shape for augmentation: {X_holdout_prepared.shape}")

# Apply noise to the holdout set with an augmentation factor of 1 (100% of samples)
X_holdout_noisy = augmenter.augment_batch(X_holdout_prepared, augmentation_factor=1.0)

# ðŸ†• Set the labels for the augmented noisy holdout set
num_original_samples = len(y_holdout)
num_augmented_samples = X_holdout_noisy.shape[0]

# Generate the correct number of labels for the augmented samples
y_holdout_noisy = np.repeat(y_holdout, np.ceil(num_augmented_samples / num_original_samples).astype(int))[:num_augmented_samples]

print(f"Noisy holdout set shape: {X_holdout_noisy.shape}")
print(f"Noisy holdout labels shape: {y_holdout_noisy.shape}")

# Load the best model
model = keras.models.load_model('best_model.h5')

# Evaluate on the noisy holdout set
print("\nEvaluating on the Noisy Holdout Set...")
y_holdout_pred = model.predict(X_holdout_noisy)
y_holdout_pred_classes = np.argmax(y_holdout_pred, axis=1)

# Calculate accuracy for the noisy holdout set
holdout_accuracy = accuracy_score(y_holdout_noisy, y_holdout_pred_classes)
print(f"Noisy Holdout Set Accuracy: {holdout_accuracy:.4f}")

# Generate classification report for the noisy holdout set
print("\nClassification Report for Noisy Holdout Set:")
print(classification_report(y_holdout_noisy, y_holdout_pred_classes, digits=4))

# Generate confusion matrix for the noisy holdout set
conf_matrix_holdout = confusion_matrix(y_holdout_noisy, y_holdout_pred_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_holdout, annot=True, cmap='Blues', fmt='d')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Noisy Holdout Set')
plt.show()









