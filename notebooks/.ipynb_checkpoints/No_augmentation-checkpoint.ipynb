{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936cd882-8e4a-494a-accb-235e99178a22",
   "metadata": {},
   "source": [
    "# ECG Classification Without Augmentation: \n",
    "This notebook explores ECG heartbeat classification using deep learning, without applying any data augmentation techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a55391-bf53-41f0-b957-1c4478e32c0c",
   "metadata": {},
   "source": [
    "## Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0648ade0-2968-41ee-9565-3f327908ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical computations\n",
    "from matplotlib import pyplot as plt  # For data visualization\n",
    "from sklearn.model_selection import train_test_split  # For splitting the dataset into train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652af6e2-32a4-4e7e-8a20-515b43ad13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ECG training dataset from a CSV file\n",
    "# The dataset does not have predefined headers, so we set `header=None`\n",
    "df_train = pd.read_csv(\"../Data/mitbih_train.csv\", header=None)\n",
    "\n",
    "# Load the ECG test dataset from a CSV file\n",
    "# This dataset will be used for evaluating the model's performance\n",
    "df_test = pd.read_csv(\"../Data/mitbih_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5983a505-5616-4dfe-83eb-987b6d4a84ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (87554, 188) (rows, columns)\n",
      "Test dataset shape: (21892, 188) (rows, columns)\n"
     ]
    }
   ],
   "source": [
    "# Check the Dimension of the Set\n",
    "print(f\"Training dataset shape: {df_train.shape} (rows, columns)\")\n",
    "\n",
    "print(f\"Test dataset shape: {df_test.shape} (rows, columns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93096ccc-0ab6-45fa-9235-1e6ea0fab082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Label distribution in Training Set:\n",
      "187\n",
      "0.0    72471\n",
      "4.0     6431\n",
      "2.0     5788\n",
      "1.0     2223\n",
      "3.0      641\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ”¹ Label distribution in Test Set:\n",
      "187\n",
      "0.0    18118\n",
      "4.0     1608\n",
      "2.0     1448\n",
      "1.0      556\n",
      "3.0      162\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”¹ Label distribution in Training Set:\")\n",
    "print(df_train.iloc[:, -1].value_counts())\n",
    "\n",
    "print(\"\\nðŸ”¹ Label distribution in Test Set:\")\n",
    "print(df_test.iloc[:, -1].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f841b0d-155d-4320-ab1d-994b353d4fcc",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab423a61-d38a-4e12-9754-238b19bf4b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minority classes: [3.0, 1.0, 2.0]\n",
      "Class distribution in the training set:\n",
      "187\n",
      "3.0      641\n",
      "1.0     2223\n",
      "2.0     5788\n",
      "4.0     6431\n",
      "0.0    69846\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Add the 'src' directory to the Python path so we can import custom utility functions\n",
    "sys.path.append(\"../src\")  \n",
    "\n",
    "# Import the stratified sampling function from utils.py\n",
    "from utils import stratified_sample  \n",
    "\n",
    "\n",
    "df_validation , df_train = stratified_sample(df_train, 3)\n",
    "\n",
    "X_train = df_train.iloc[:, :-1]\n",
    "y_train = df_train.iloc[:, -1]\n",
    "\n",
    "X_validation = df_validation.iloc[:, :-1]\n",
    "y_validation = df_validation.iloc[:,-1]\n",
    "\n",
    "\n",
    "# X_holdout = X_holdout.reset_index(drop=True)\n",
    "# y_holdout = y_holdout.reset_index(drop=True)\n",
    "# Identify the distribution of classes in the training set\n",
    "class_distribution = y_train.value_counts().sort_values()\n",
    "\n",
    "# Select the three least represented classes (minority classes)\n",
    "minority_classes = class_distribution.index[:3]\n",
    "\n",
    "# Print the identified minority classes\n",
    "print(f\"Minority classes: {minority_classes.tolist()}\")\n",
    "\n",
    "# Display the class distribution in the training set\n",
    "# This helps assess dataset imbalance and decide on augmentation strategies\n",
    "print(\"Class distribution in the training set:\")\n",
    "print(class_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c050f32-ef03-4969-a7cf-58c020716495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "0.0    525\n",
      "3.0    525\n",
      "2.0    525\n",
      "4.0    525\n",
      "1.0    525\n",
      "Name: count, dtype: int64\n",
      "187\n",
      "0.0    69846\n",
      "4.0     6431\n",
      "2.0     5788\n",
      "1.0     2223\n",
      "3.0      641\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the class distribution in the validation set\n",
    "# This helps verify that stratified sampling preserved the class balance\n",
    "print(y_validation.value_counts())\n",
    "\n",
    "# Print the class distribution in the training set\n",
    "# Ensures we have a clear understanding of how many samples exist for each class\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3611c1c1-48e1-413e-a2b9-2248ca97e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minority classes: [3.0, 1.0, 2.0]\n",
      "Class distribution in the training set:\n",
      "187\n",
      "3.0      641\n",
      "1.0     2223\n",
      "2.0     5788\n",
      "4.0     6431\n",
      "0.0    69846\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify the minority classes\n",
    "class_distribution = y_train.value_counts().sort_values()\n",
    "minority_classes = class_distribution.index[:3]\n",
    "print(f\"Minority classes: {minority_classes.tolist()}\")\n",
    "print(\"Class distribution in the training set:\")\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d81d65-a4b8-407b-8893-2a67f6781abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape for model: (84929, 187, 1)\n",
      "Labels shape: (84929,)\n"
     ]
    }
   ],
   "source": [
    "#%% Prepare Data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from cnn_lstm_classifier import CNNLSTMClassifier  # Assuming the model class is in the src directory\n",
    "\n",
    "X_data = np.expand_dims(X_train.values, axis=-1)  # Add channel dimension for CNN\n",
    "y_data = y_train.values\n",
    "\n",
    "print(f\"Input shape for model: {X_data.shape}\")\n",
    "print(f\"Labels shape: {y_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ddeb48-9503-4d4c-84d3-2721ec4a99a8",
   "metadata": {},
   "source": [
    "## Model Selection & Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a91af950-fe1d-47bd-81ff-576c034e3401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow import keras\n",
    "from data_augmentation import DataAugmentation  # Assuming the class is in the src folder\n",
    "from cnn_lstm_classifier import CNNLSTMClassifier  # Assuming the model class is in the src directory\n",
    "\n",
    "\n",
    "# To store evaluation metrics\n",
    "accuracy_scores = []\n",
    "classification_reports = []\n",
    "confusion_matrices = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee6e3cb-e1a9-4f03-8b56-7dbbf192c8c8",
   "metadata": {},
   "source": [
    "## Initialize the model and training : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed4b5c91-9422-4a1d-bda1-2d59f3a46e5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize the CNN + LSTM model with specified parameters\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m CNNLSTMClassifier(\n\u001b[1;32m      9\u001b[0m     input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m187\u001b[39m, \u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# Input shape matches the ECG signal length and single-channel format\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_test)),  \u001b[38;5;66;03m# Automatically determine the number of classes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m,  \u001b[38;5;66;03m# Set an appropriate learning rate for training\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,  \u001b[38;5;66;03m# Define batch size for training\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# Set the number of epochs for training\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model using the training dataset\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# This step involves feeding the training data to the model and optimizing weights\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for model evaluation and visualization\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize the CNN + LSTM model with specified parameters\n",
    "model = CNNLSTMClassifier(\n",
    "    input_shape=(187, 1),  # Input shape matches the ECG signal length and single-channel format\n",
    "    num_classes=len(np.unique(y_data)),  # Automatically determine the number of classes\n",
    "    learning_rate=1e-3,  # Set an appropriate learning rate for training\n",
    "    batch_size=32,  # Define batch size for training\n",
    "    epochs=50  # Set the number of epochs for training\n",
    ")\n",
    "\n",
    "# Train the model using the training dataset\n",
    "# This step involves feeding the training data to the model and optimizing weights\n",
    "print(\"\\nStarting model training...\")\n",
    "model.fit(X_data, y_data, X_validation, y_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5907ca1-ee1e-46a8-8086-24b4267b3df8",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae89946-09c7-4dc2-b49e-aa6c5380bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss curves to visualize the training process\n",
    "# Helps in identifying underfitting or overfitting\n",
    "model.plot_history()\n",
    "\n",
    "# Evaluate the model using the validation dataset\n",
    "# This provides insights into how well the model generalizes to unseen data\n",
    "print(\"\\nEvaluating on Validation Set...\")\n",
    "model.evaluate(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d584b1-dae6-4cd9-8af2-4fcab75d4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model to make predictions\n",
    "best_model = keras.models.load_model('no_augmentation_saved_model.h5')\n",
    "\n",
    "# Predict on the validation set using the best model\n",
    "y_val_pred = best_model.predict(X_validation)\n",
    "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = accuracy_score(y_validation, y_val_pred_classes)\n",
    "print(f\"\\nValidation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate and display classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_validation, y_val_pred_classes, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_validation, y_val_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9793a4f-1271-42e0-84b1-c0d31c04db7a",
   "metadata": {},
   "source": [
    "## test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f1ee2-13db-4b32-8358-53b22a384ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evlauate the model with the test set\n",
    "\n",
    "# Ensure the test data has the correct shape\n",
    "X_test = np.expand_dims(X_test, axis=-1)  # Add channel dimension if not already added\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the best model weights\n",
    "#best_model = keras.models.load_model('best_model.h5')\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_test_pred_classes, digits=4))\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred_classes)\n",
    "print(f\"Accuracy on Test Set: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix on Test Set')\n",
    "plt.savefig(\"Confusion Matrix on Test Set\" , bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553b7e7-d7f3-47b5-8cc3-d4431c8da33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e85e45-e26a-4f41-8e84-cc970d64a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_augmentation import DataAugmentation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "\n",
    "# Initialize the Data Augmentation class for noise injection\n",
    "augmenter = DataAugmentation(\n",
    "    shift_max=0,       # No time shift\n",
    "    noise_level=0.1,   # Introduce significant noise\n",
    "    scale_range=(1.0, 1.0), # No scaling\n",
    "    expected_length=187\n",
    ")\n",
    "\n",
    "# ðŸ†• Prepare holdout data with the channel dimension for augmentation\n",
    "X_holdout_prepared = np.expand_dims(X_holdout, axis=-1)  # Shape should be (samples, 187, 1)\n",
    "y_holdout = y_holdout\n",
    "\n",
    "#%%\n",
    "print(f\"Prepared holdout set shape for augmentation: {X_holdout_prepared.shape}\")\n",
    "\n",
    "# Apply noise to the holdout set with an augmentation factor of 1 (100% of samples)\n",
    "X_holdout_noisy = augmenter.augment_batch(X_holdout_prepared, augmentation_factor=1.0)\n",
    "\n",
    "# ðŸ†• Set the labels for the augmented noisy holdout set\n",
    "num_original_samples = len(y_holdout)\n",
    "num_augmented_samples = X_holdout_noisy.shape[0]\n",
    "\n",
    "# Generate the correct number of labels for the augmented samples\n",
    "y_holdout_noisy = np.repeat(y_holdout, np.ceil(num_augmented_samples / num_original_samples).astype(int))[:num_augmented_samples]\n",
    "\n",
    "print(f\"Noisy holdout set shape: {X_holdout_noisy.shape}\")\n",
    "print(f\"Noisy holdout labels shape: {y_holdout_noisy.shape}\")\n",
    "\n",
    "# Load the best model\n",
    "#model = keras.models.load_model('best_model.h5')\n",
    "\n",
    "# Evaluate on the noisy holdout set\n",
    "print(\"\\nEvaluating on the Noisy Holdout Set...\")\n",
    "y_holdout_pred = model.predict(X_holdout_noisy)\n",
    "y_holdout_pred_classes = np.argmax(y_holdout_pred, axis=1)\n",
    "\n",
    "# Calculate accuracy for the noisy holdout set\n",
    "holdout_accuracy = accuracy_score(y_holdout_noisy, y_holdout_pred_classes)\n",
    "print(f\"Noisy Holdout Set Accuracy: {holdout_accuracy:.4f}\")\n",
    "\n",
    "# Generate classification report for the noisy holdout set\n",
    "print(\"\\nClassification Report for Noisy Holdout Set:\")\n",
    "print(classification_report(y_holdout_noisy, y_holdout_pred_classes, digits=4))\n",
    "\n",
    "# Generate confusion matrix for the noisy holdout set\n",
    "conf_matrix_holdout = confusion_matrix(y_holdout_noisy, y_holdout_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_holdout, annot=True, cmap='Blues', fmt='d')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Noisy Holdout Set')\n",
    "plt.savefig(\"Confusion Matrix for Noisy Holdout Set\" , bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e2256-43ed-4a43-8375-8273b5899401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "sys.path.append('../src')\n",
    "from cnn_lstm_classifier import CNNLSTMClassifier  # Assuming the model class is in the src directory\n",
    "\n",
    "# Create model instance\n",
    "model = CNNLSTMClassifier(input_shape=(187, 1), num_classes=5)\n",
    "\n",
    "# Build the model explicitly before plotting\n",
    "model.model.build(input_shape=(None, 187, 1))\n",
    "\n",
    "# Plot the model architecture\n",
    "plot_model(model.model, to_file=\"model_plot.png\", show_shapes=True, show_layer_names=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
